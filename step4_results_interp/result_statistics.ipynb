{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7609246a-d080-463d-bb79-d9f6a9261fc1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# ML Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a5c3e-6681-4778-8538-ad25ca73b122",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3582168-aca2-4941-8983-a54ec04fd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from itertools import combinations, permutations\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlite3 import connect\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "logger = logging.getLogger(\"ResultAnalysis\")\n",
    "\n",
    "np.random.seed(707260)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccecd4b-f694-44b7-bbd0-e0c410dd23ee",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9fadcd-3bd7-43ee-99c4-0aae73f837b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connects to a database, and loads all contents into a single dataframe\n",
    "def parse_results_db(sqlite_path: Path):\n",
    "    # Establish a \"connection\" (file I/O) to the database\n",
    "    db_con = connect(sqlite_path)\n",
    "    db_cur = db_con.cursor()\n",
    "\n",
    "    # Get a list of the tables present in the database (should be one for every analysis)\n",
    "    result_tables = db_cur.execute(\"SELECT name FROM sqlite_master\").fetchall()\n",
    "\n",
    "    # Unpack the list of (single-value) tuples into a single listba\n",
    "    result_tables = [x[0] for x in result_tables]\n",
    "\n",
    "    # Read the contents of each results table into a DataFrame\n",
    "    result_dfs = []\n",
    "    for t in result_tables:\n",
    "        try:\n",
    "            df = pd.read_sql(\n",
    "                f\"SELECT * FROM {t}\",\n",
    "                con=db_con\n",
    "            )\n",
    "        # Occasionally, tables will get corrupted if they were being written to when a job is terminated (i.e. SLURM job cancellation)\n",
    "        except:\n",
    "            logging.warning(f\"Failed to read table '{t}', ignoring it\")\n",
    "            continue\n",
    "\n",
    "        # Add a column tracking the study, model, and dataset\n",
    "        df.loc[:, [\"study\", \"model\", \"dataset\"]] = t.split('__')\n",
    "\n",
    "        # Add it to the list\n",
    "        result_dfs.append(df)\n",
    "\n",
    "    # Concatenate the results into one large dataframe and return it\n",
    "    return pd.concat(result_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d171d-687a-46c1-88ac-f9d0afab98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = parse_results_db(Path(\"../step3_run_analysis/results/dcm_classic_ml.db\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed53f5c-2bdd-42e0-9bab-20a669f0c71b",
   "metadata": {},
   "source": [
    "### Analysis Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac4414-0c26-47d5-b11f-80cf32f112aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_indices = ['feature_type', 'feature_set', 'scope', 'mri_type', 'algorithm', 'data_prep']\n",
    "\n",
    "# Sub-index helpers\n",
    "def get_feature_type(dataset_components: list[str]):\n",
    "    return dataset_components[0]\n",
    "\n",
    "# Unique to datasets containing only clinical metrics\n",
    "def get_prep_clin(dataset_components: list[str]):\n",
    "    return '_'.join(dataset_components[1:])\n",
    "\n",
    "# Unique to datasets w/ MRI-containing metrics\n",
    "def get_feature_set(dataset_components: list[str]):\n",
    "    return dataset_components[2]\n",
    "\n",
    "def get_scope(dataset_components: list[str]):\n",
    "    return '_'.join(dataset_components[3:5])\n",
    "\n",
    "def get_mri_type(dataset_components: list[str]):\n",
    "    return '_'.join(dataset_components[5:7])\n",
    "\n",
    "def get_algorithm(dataset_components: list[str]):\n",
    "    return dataset_components[7]\n",
    "\n",
    "def get_prep_img(dataset_components: list[str]):\n",
    "    return '_'.join(dataset_components[8:])\n",
    "\n",
    "# Helper function make dataframe application easier\n",
    "def parse_dataset(dataset_label: str):\n",
    "    dataset_components = dataset_label.split('_')\n",
    "    feature_type = get_feature_type(dataset_components)\n",
    "    if feature_type == 'clinical':\n",
    "        prep = get_prep_clin(dataset_components)\n",
    "        return (feature_type, 'N/A', 'N/A', 'N/A', 'N/A', prep)\n",
    "    else:\n",
    "        feature_set = get_feature_set(dataset_components)\n",
    "        scope = get_scope(dataset_components)\n",
    "        mri_type = get_mri_type(dataset_components)\n",
    "        algorithm = get_algorithm(dataset_components)\n",
    "        prep = get_prep_img(dataset_components)\n",
    "        return np.array([feature_type, feature_set, scope, mri_type, algorithm, prep])\n",
    "\n",
    "# Parse the dataset column to form our \"analysis\" columns\n",
    "new_vals = np.stack(results_df['dataset'].apply(parse_dataset))\n",
    "results_df.loc[:, data_indices] = new_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864003b1-4575-40ec-94e9-d8068d675385",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[data_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3803e-17db-4177-b4ef-2242c9775cf2",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3b367-e4ad-4a47-b7a7-608cb47e858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis index; used in grouping operations to gather samples of the sample \"methdology\"\n",
    "analysis_idx = [\"study\", \"model\", *data_indices]\n",
    "# Same as above, with replicates (which allow for variation calculations\n",
    "analysis_idx_w_replicates = [*analysis_idx, \"replicate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec80a422-63b3-41fe-a2fc-2c0df487253c",
   "metadata": {},
   "source": [
    "## Best Performance Across Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6053d950-9cfd-4406-97fe-10ac3d635822",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2feae-fccf-4fb6-a9cb-29d92ca53bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_at_other_optima(grouping_cols: list[str], other_cols: list[str], ascending: list[bool], df: pd.DataFrame, n=1):\n",
    "    # Sort the dataframe by the \"other\", placing their optima (as desingated by the user) towards the bottom\n",
    "    sorted_df = df.sort_values(by=other_cols, ascending=ascending)\n",
    "\n",
    "    # Group the results by our grouping indices, and grab the last n entries (which correspond to our optima)\n",
    "    optima_df = sorted_df.groupby(grouping_cols).tail(n)\n",
    "\n",
    "    # Return the result\n",
    "    return optima_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925978e4-0cd1-47ab-a231-da06cb3684a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metric_report(target_col: str, df: pd.DataFrame):\n",
    "    # Convert this to floating point, to suppress \"cannot apply to object type\" errors\n",
    "    tmp_df = df.set_index(analysis_idx)\n",
    "    tmp_df[target_col] = tmp_df[target_col].astype('float32')\n",
    "\n",
    "    # Calculate the mean and standard deviation of the model's performance\n",
    "    target_metric_means = tmp_df.groupby(analysis_idx)[target_col].mean()\n",
    "    target_metric_stds = tmp_df.groupby(analysis_idx)[target_col].std()\n",
    "\n",
    "    # Place it into a dataframe for easier management\n",
    "    report_df = pd.DataFrame(\n",
    "        columns=[\"MEAN\", \"STD\"],\n",
    "        index=target_metric_means.index\n",
    "    )\n",
    "\n",
    "    report_df[\"MEAN\"] = target_metric_means\n",
    "    report_df[\"STD\"] = target_metric_stds\n",
    "\n",
    "    # Sort them in ascending order\n",
    "    report_df = report_df.sort_values(\"MEAN\")\n",
    "\n",
    "    # Return the report\n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b622b9-258a-4e12-8599-e94200f5deb9",
   "metadata": {},
   "source": [
    "### Testing Balanced Accuracy @ Peak Validation Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d010367-363d-42ea-b298-4ee89e16875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_cols = ['balanced_accuracy (validate)', 'log_loss (validate)']\n",
    "# validation_optima_dir = [False, True]\n",
    "validation_optima_dir = [True, False]\n",
    "target_metric = 'balanced_accuracy (test)'\n",
    "\n",
    "bacc_validation_optima_df = get_values_at_other_optima(grouping_cols=analysis_idx_w_replicates, other_cols=sorting_cols, ascending=validation_optima_dir, df=results_df)\n",
    "\n",
    "build_metric_report(target_metric, bacc_validation_optima_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0954b32-1bcf-4f5d-a429-aac2ead31deb",
   "metadata": {},
   "source": [
    "### Testing Balanced Accuracy @ Minimum Log-Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94ee81-fe31-4ca3-897a-676faf88ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_cols = ['log_loss (validate)', 'balanced_accuracy (validate)']\n",
    "validation_optima_dir = [False, True]\n",
    "# validation_optima_dir = [True, False]\n",
    "target_metric = 'balanced_accuracy (test)'\n",
    "\n",
    "log_loss_validation_optima_df = get_values_at_other_optima(grouping_cols=analysis_idx_w_replicates, other_cols=sorting_cols, ascending=validation_optima_dir, df=results_df)\n",
    "\n",
    "build_metric_report(target_metric, log_loss_validation_optima_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e2105-93cf-46a2-af1c-1f29252b55a4",
   "metadata": {},
   "source": [
    "## Statistical Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d80ea-006c-4ecd-b2c0-cee3b3a15fb5",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d508b4-6b47-48b2-a841-cba24afa6563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "from scipy.stats import ranksums, kruskal, false_discovery_control\n",
    "\n",
    "alt_keys = {\n",
    "    'two-sided': '!=',\n",
    "    'greater':   '>',\n",
    "    'less':      '<'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da665864-5de9-48b3-9622-f1fe4e9f9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_rankedsum(df: pd.DataFrame, query: list[str], target: str, alternative: str = 'two-sided'):\n",
    "    pvals = {}\n",
    "    query_set = set(df[query])\n",
    "\n",
    "    # Caclulate the native rankedsum p-value for each pair of datasets, testing whether the former's value is greater than the latters\n",
    "    for v1, v2 in permutations(query_set, 2):\n",
    "        x1 = df.query(f\"{query} == '{v1}'\")[target]\n",
    "        x2 = df.query(f\"{query} == '{v2}'\")[target]\n",
    "        p = ranksums(x1, x2, alternative=alternative).pvalue\n",
    "        pvals[f\"{v1} {alt_keys[alternative]} {v2} [{query}]\"] = [p]\n",
    "\n",
    "    # Save the results as a dataframe\n",
    "    return_df = pd.DataFrame.from_dict(pvals).T\n",
    "    return_df.index.name = 'Comparison'\n",
    "    return_df.columns = ['p']\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8d4e9-b6a7-4833-a025-00a65885c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the p-values for whether one experimental permutation has greater average balanced accuracy (testing) than another\n",
    "sub_dfs = []\n",
    "target = 'balanced_accuracy (test)'\n",
    "for k in analysis_idx:\n",
    "    if len(set(results_df[k])) < 2:\n",
    "        logger.warning(f\"Column '{k}' was homogenous, cannot split for statistical comparisons!\")\n",
    "        continue\n",
    "    tmp_df = paired_rankedsum(bacc_validation_optima_df, k, target, alternative='greater')\n",
    "    sub_dfs.append(tmp_df)\n",
    "\n",
    "sig_test_at_peak_valid_df = pd.concat(sub_dfs).sort_values('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019250ba-3588-4d81-9916-541b2d92dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_test_at_peak_valid_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a413e-3f9c-465f-b075-64707326d3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
